\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{epstopdf}
%\usepackage[english]{babel}
\usepackage[latin5]{inputenc}
\usepackage{hyperref}
\usepackage[left=3cm,top=3cm,right=3cm,nohead,nofoot]{geometry}
\usepackage{braket}
\usepackage{datenumber}
%\newdate{date}{10}{05}{2013}
%\date{\displaydate{date}}

\begin{document}

\begin{center}
\Huge
Redes neuronales artificiales desde un enfoque físico

\vspace{3mm}
\Large Jesus David Prada Gonzalez

\large
201214619


\vspace{2mm}
\Large
Director: Jaime E. Forero-Romero

\normalsize
\vspace{2mm}

\today
\end{center}


\normalsize
\section{Introduction}

%Introducción a la propuesta de Monografía. Debe incluir un breve resumen del estado del arte del problema a tratar. También deben aparecer citadas todas las referencias de la bibliografía (a menos de que se citen más adelante, en los objetivos o metodología, por ejemplo)

La inteligencia artificial es uno de los temas que han fascinado a la humanidad desde el desarrollo de la computación. Ésta ha sido tema recurrente de muchas de las historias modernas de ciencia ficción, y con los avances de la tecnología moderna en términos de poder computacional, esta idea se hace cada vez más palpable. Es por esto que la inteligencia artificial Multivac de Asimov es un personaje recurrente en sus grandiosos cuentos futuristas \cite{Asimov}.\\

Uno de los aspectos de la inteligencia artificial que está al alcance de nuestras manos hoy en día, es el aprendizaje de máquinas, más conocido como Machine Learning. Éste hace referencia a la capacidad de las máquinas para aprender a enfrentarse a ciertos problemas sin ser específicamente programadas para ello. Ésto se logra por una variación de sus parámetros en lo que se conoce como aprendizaje supervisado (Supervised Learning), en donde se tiene una función de fitness o verosimilitud que indica qué tan acertado es el enfoque de la máquina al problema. Con varios intentos en el proceso de aprendizaje, la máquina puede calcular el gradiente aproximado de dicho fitness respecto a sus parámetros y modificarlos de tal manera que se optimice dicha función \cite{deepNature}. Así, si se entrena la máquina con un conjunto de datos conocidos, esta optimizará sus parámetros para poder enfrentarse a problemas nuevos de manera óptima.\\

Las dificultades de la programación de una inteligencia artificial se hacen claras al intentar programar, por ejemplo, un identificador de números en una imagen \cite{neuralOnline}. Lo que parece muy intuitivo para nosotros, como la identificación de curvas, contornos, intersecciones, etc. se vuelve un poco complicado al intentar expresarlo en un programa. De repente nos encontramos intentando clasificar casos, que en general son muy numerosos y harían el código muy ineficiente. De esta manera, deducimos que nuestra red neuronal, compuesta de diferentes capas de neuronas relacionadas entre sí, es óptima para enfrentarse a estos problemas, por esto uno de los abordajes más exitosos para Machine Learning reside en la mímica computacional de las redes neuronales biológicas. De esta manera se pueden crear redes neruonales artificiales que procesen la información de imágenes para identificar objetos, rostros, animales, etc. Incluso, se pueden programar para que éstas aprendan a jugar videojuegos y más \cite{statisticNeurons}. Fue de esta manera que se pudo crear una inteligencia artificial capaz de jugar Go y de vencer al campeón mundial.\\

A grandes rasgos, una neurona puede verse como un nodo que se activa o no dependiendo de unos estímulos de entrada. Computacionalmente esto se puede traducir a un output binario que se da como consecuencia de la aplicación de cierta función a unos inputs binarios. Los parámetros de dicha función serían los optimizados por el proceso de Supervised Learning. Teniendo esto en cuenta, una red de neuronas se forma al tomar como input de unas neuronas, el output de otras, formando así diferentes capas de procesamiento.\\

Lo curioso de estas redes neuronales es que son extremadamente eficientes al enfrentarse a ciertos problemas a pesar de que no se conoce muy bien el mecanismo de su éxito. Se sabe que ciertos diseños de redes funcionan de manera óptima para ciertos problemas, pero estas son conclusiones obtenidas heurísticamente. De esta manera, resulta importante poder cuantificar el proceso de aprendizaje de una neurona para monitorear su proceso de aprendizaje y poder obtener conclusiones de su mecanismo. Ya es conocido que muchas analogías de la mecánica estadística puede ser aplicadas al funcionamiento de las redes neuronales artificiales \cite{cheapLearning} y de esta manera se puede entender a grandes rasgos por qué funcionan al enfrentarse a problemas tan generales. De la misma manera, hay modelos estadísticos sobre redes neuronales biológicas \cite{biology}, que pueden ser usados análogamente para modelar redes neuronales artificiales.\\

Consecuentemente, el objetivo de este proyecto de grado está en el estudio de modelos teóricos sobre redes neuronales artificiales para poder obtener conclusiones sobre el mecanismo de su éxito y verificarlo y aplicarlo a redes neruronales artificiales enfrentándose a problemas reales.\\

\section{Objetivos Generales}

%Objetivo general del trabajo. Empieza con un verbo en infinitivo.

Estudiar el comportamiento de las redes neuronales desde un enfoque físico para verificar si el mecanismo de su aprendizaje es verificable por medio de modelos teóricos.

\section{Objetivos Específicos}

%Objetivos específicos del trabajo. Empiezan con un verbo en infinitivo.

\begin{itemize}
	\item Desarrollar redes neuronales simples para familiarizarse con su diseño
	\item Estudiar a fondo las técnicas computacionales de diseño de redes neuronales, así como los problemas técnicos a los que se pueden enfrentar
	\item Elegir entre las diferentes aplicaciones de redes neuronales una que sea versátil para el estudio de los modelos físicos asociados
	\item Estudiar qué posibles modelos físicos y estadísticos pueden ser aplicados a la red neuronal elegida para poder obtener conclusiones acerca de su funcionamiento
	\item Verificar las aplicaciones de estos modelos sobre la red elegida.
	\item Verificar la generalización de estos modelos para redes neuronales con diseños diferentes
\end{itemize}

\section{Metodología}

%Exponer DETALLADAMENTE la metodología que se usará en la Monografía. 

%Monografía teórica o computacional: ¿Cómo se harán los cálculos teóricos? ¿Cómo se harán las simulaciones? ¿Qué requerimientos computacionales se necesitan? ¿Qué espacios físicos o virtuales se van a utilizar?

%Monografía experimental: Recordar que para ser aprobada, los aparatos e insumos experimentales que se usarán en la Monografía deben estar previamente disponibles en la Universidad, o garantizar su disponibilidad para el tiempo en el que se realizará la misma. ¿Qué montajes experimentales se van a usar y que material se requiere? ¿En qué espacio físico se llevarán a cabo los experimentos? Si se usan aparatos externos, ¿qué permisos se necesitan? Si hay que realizar pagos a terceros, ¿cómo se financiará esto?

El estudiante realizará la investigación y el desarrollo de los temas individualmente, con el apoyo periódico del director por medio de las reuniones del grupo de Astrofísica. En estas obtendrá retroalimentación del desarrollo de su trabajo y se decidirá si en algún momento requiere de mayor asesoría por parte del director.\\

La metodología propuesta tiene un componente computacional fuerte relacionado con el desarrollo de varias redes neuronales y el monitoreo de su proceso de aprendizaje para la verificación de las conclusiones teóricas a obtener. A lo sumo, para agilizar los procesos de cómputo, se necesitará acceso al Cluster. Para el desarrollo de la parte teórica será necesaria la revisión de bibliografía especializada sobre el tema. \\


\section{Cronograma}

\begin{table}[htb]
	\begin{tabular}{|c|cccccccccccccccc| }
	\hline
	Tareas $\backslash$ Semanas & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16  \\
	\hline
	1 & X & X & X & X & X & X &   &   &   &   &   &   &   &   &   &   \\
	2 & X & X & X & X & X & X & X & X & X &   &   &   &   &   &   &   \\
	3 &   &   &   &   &   &   &   & X & X & X & X & X & X & X & X & X \\
	4 &   &   &   &   &   &   &   &   &   &   &   & X & X & X & X & X \\
	5 &   &   &   &   &   &   &   &   &   & X & X & X & X & X & X & X \\
	9 &   &   &   &   & X & X & X & X & X &   &   &   & X & X & X & X \\

	
	Tareas $\backslash$ Semanas & 17 & 18 & 19 & 20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32  \\
	\hline
	4 & X & X & X & X & X & X &   &   &   &   &   &   &   &   &   &   \\
	6 &   &   & X & X & X & X & X & X &   &   &   &   &   &   &   &   \\
	7 &   &   &   &   &   & X & X & X & X & X & X &   &   &   &   &   \\
	8 &   &   &   &   &   &   &   &   & X & X & X & X & X &   &   &   \\
	9 & X & X & X & X & X & X & X & X & X & X & X & X & X & X & X & X \\


	\hline
	\end{tabular}
\end{table}
\vspace{1mm}

\begin{itemize}
	\item Tarea 1: Desarrollo de redes neuronales simples
	\item Tarea 2: Investigación del diseño y funcionamiento de redes neuronales
	\item Tarea 3: Investigación de modelos teóricos sobre redes neuronales 
	\item Tarea 4: Desarrollo de intuiciones o modelos verificables sobre el funcionamiento del aprendizaje de las redes
	\item Tarea 5: Preparación de la presentación de avance
	\item Tarea 6: Verificación de las conclusiones de los modelos anteriores sobre las redes neuronales artificiales específicas
	\item Tarea 7: Ampliación a redes neuronales más complejas
	\item Tarea 8: Desarrollo de resultados y conclusiones del trabajo
	\item Tarea 9: Escritura del documento final
\end{itemize}

\section{Personas Conocedoras del Tema}

%Nombres de por lo menos 3 profesores que conozcan del tema. Uno de ellos debe ser profesor de planta de la Universidad de los Andes.

\begin{itemize}
	\item Juan Manuel Pedraza (Universidad de los Andes)
	\item Alonso Botero (Universidad de los Andes)
	\item Juan Gabriel Ramírez (Universidad de los Andes)
\end{itemize}


\begin{thebibliography}{10}
\bibitem{Asimov} Asimov, I. (1974). The best of Isaac Asimov. Garden City, NY: Doubleday. 

\bibitem{deepNature} Lecun, Y., Bengio, Y., \& Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. DOI: 10.1038/nature14539 

\bibitem{neuralOnline} M. (2016, January). Neural networks and deep learning. Retrieved November 04, 2016, from http://neuralnetworksanddeeplearning.com/

\bibitem{statisticNeurons} Lin, H.~W., \& Tegmark, M. (2016). Why does deep and cheap learning work so well? \  arXiv:1608.08225 

\bibitem{biology} Schneidman, E., Berry, M. J., Segev, R., \& Biealek, W. (2006, April 20). Weak pairwise correlations imply strongly correlated network states in a neural population. Nature, 440, 1007-1012.

\end{thebibliography}

\section*{Firma del Director}
\vspace{1.5cm}

\section*{Firma del Estudiante}



\end{document} 